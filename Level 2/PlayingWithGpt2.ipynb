{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1201f50b-07f1-42a0-b8c0-e18a4502ecf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, you could see it as a small, dim light in the darkness.\n",
      "\n",
      "But now, as you were moving forward, you heard it as it approached, and as it passed, it stopped, and it began to rise\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the model and tokenizer from the local directory\n",
    "model_name = \"./gpt2_local\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Test the model with some input\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Set pad_token_id to avoid warnings\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Create the attention mask\n",
    "attention_mask = input_ids.ne(pad_token_id).long()\n",
    "\n",
    "# Use top-k and top-p sampling\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    top_k=10,  # Top-k sampling\n",
    "    top_p=0.9,  # Top-p (nucleus) sampling\n",
    "    do_sample=True,  # Ensures sampling instead of greedy decoding\n",
    "    pad_token_id=pad_token_id,  # Set the pad_token_id\n",
    "    attention_mask=attention_mask  # Provide the attention mask\n",
    ")\n",
    "\n",
    "# Decode and print the output text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9536dc60-eac2-4dd5-9d23-669298eb7351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "PyTorch version: 2.0.1\n",
      "Transformers version: 4.30.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd07a007-317f-4c50-ab35-0cedc6c0d283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God does not exist  2\n",
      "\n",
      "(2) A man may take one or more of the following: (a) a small baggie of water; (b) a cup of water; (c) a small bag of tea; (d) a cup of milk; (e) an open jar of water; (f) a small piece of bread or small bag of flour; (g) a bowl of hot sauce; or (h) a bowl of cheese; or (i) a bottle or cup containing an amount equal to three grams of water per gallon of water. (4) A woman may use the word \"sugar,\" \"flour,\" or \"oil,\" to mean a beverage that does not contain sugar or any other liquid or substance, but is prepared by boiling water. (5) An alcoholic beverage is a container that has been placed upon the container by the user or container is in an open container, if the person who made the beverage is a registered user or registered holder. (6) A person shall not have more than one person in any room, in any building or any place in which the person who makes the beverage is an employee of any of the following: a manufacturer or distributor of a food product for which the alcoholic beverage has been made; or a distributor of a food or beverage in any retail trade that does not carry or provide food, drink or beverage that is made from food, drink or drink beverages; (b) any of the following food and beverage companies: a food or beverage distributor, the manufacturer, distributor or manufacturer's or manufacturer's employee or the consumer representative of any person who has or will have the right to possess or supply food, drink or wine and who does not possess the right to manufacture or sell such food or beverage, or who will or who will not have the right to sell any such food or beverage or who has a monopoly or license in such a business that does not provide for a monopoly, unless the person who makes or offers for sale such a food or beverage is registered with the board.\n",
      "\n",
      "Effective Date: 04-02-1999; 09-31-2001; 2013.\n",
      "\n",
      "An individual who does not own a liquor store is not allowed to consume a single alcoholic beverage on the premises, even if there are four or five people who have purchased alcohol in their store.\n",
      "\n",
      "Effective History: 2003; 2012, c. 5, s. 7.\n",
      "\n",
      "Annotation The statute requires that\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # You can also use \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_prompt(seed_text=\"\"):\n",
    "    # Encode input prompt\n",
    "    input_ids = tokenizer.encode(seed_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=500,  # Adjust length of generated text\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=5,\n",
    "        top_k=40,\n",
    "        top_p=0.5,\n",
    "        temperature=1.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    seed = \" \"\n",
    "    prompt = generate_prompt(seed)\n",
    "    print(\"One can only wish\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e262709b-5319-484c-b8c7-e19b0c2f3043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  a cat is a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word-by-word probability distribution:\n",
      "'a': 0.0023\n",
      "' cat': 0.0006\n",
      "' is': 0.0001\n",
      "' a': 0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  a dog is the\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word-by-word probability distribution:\n",
      "'a': 0.0023\n",
      "' dog': 0.0014\n",
      "' is': 0.0001\n",
      "' the': 0.0003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt (or 'quit' to exit):  quit\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def get_word_probabilities(model, tokenizer, prompt):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Get the model's output\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = torch.softmax(logits[0], dim=-1)\n",
    "    \n",
    "    # Get the tokens and their probabilities\n",
    "    word_probs = []\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        word = tokenizer.decode(input_ids[0][i])\n",
    "        prob = probs[i][input_ids[0][i]].item()\n",
    "        word_probs.append((word, prob))\n",
    "    \n",
    "    return word_probs\n",
    "\n",
    "def main():\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = 'gpt2'\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    while True:\n",
    "        prompt = input(\"Enter a prompt (or 'quit' to exit): \")\n",
    "        if prompt.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        word_probs = get_word_probabilities(model, tokenizer, prompt)\n",
    "\n",
    "        print(\"\\nWord-by-word probability distribution:\")\n",
    "        for word, prob in word_probs:\n",
    "            print(f\"'{word}': {prob:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aa06e5c-b55f-49f5-9f81-74d158f18a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'The quick brown fox'\n",
      "Temperature: '5' \n",
      "Top_p: '0.5' \n",
      "Top_k: '10' \n",
      "Top-10 predictions for the next word:\n",
      "1: es (Probability: 0.2444)\n",
      "2:  was (Probability: 0.1983)\n",
      "3:  is (Probability: 0.1874)\n",
      "4: 's (Probability: 0.1860)\n",
      "5: , (Probability: 0.1839)\n",
      "6: ' (Probability: 0.0000)\n",
      "7: # (Probability: 0.0000)\n",
      "8: & (Probability: 0.0000)\n",
      "9: % (Probability: 0.0000)\n",
      "10: * (Probability: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, top_k_top_p_filtering\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the model and tokenizer from the local directory\n",
    "model_name = \"./gpt2_local\"  # Path to the locally stored GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Test the model with some input\n",
    "input_text = \"The quick brown fox\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model's output logits\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Get the logits for the last token in the input\n",
    "last_token_logits = logits[:, -1, :]\n",
    "top_k = 10\n",
    "top_p = 0.5\n",
    "#####\n",
    "temperature = 5\n",
    "last_token_logits /= temperature\n",
    "filtered_logits = top_k_top_p_filtering(last_token_logits, top_k=top_k, top_p=top_p)\n",
    "# Calculate probabilities using softmax\n",
    "probs = F.softmax(filtered_logits, dim=-1)\n",
    "\n",
    "# Get the top 10 predictions\n",
    "top_k = 10\n",
    "top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "\n",
    "\n",
    "print(f\"Input: '{input_text}'\")\n",
    "print(f\"Temperature: '{temperature}' \")\n",
    "print(f\"Top_p: '{top_p}' \")\n",
    "print(f\"Top_k: '{top_k}' \")\n",
    "print(f\"Top-{top_k} predictions for the next word:\")\n",
    "\n",
    "for i, (index, prob) in enumerate(zip(top_k_indices[0], top_k_probs[0])):\n",
    "    predicted_word = tokenizer.decode([index])\n",
    "    print(f\"{i+1}: {predicted_word} (Probability: {prob.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516601a-305a-4857-9d81-5d2bc77f4b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
